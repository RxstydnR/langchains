{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a7f6e4-d5e2-4dc4-a21b-928f88be0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad804158-cf85-480a-8030-1d9e61b641b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator = \"\",  # セパレータ\n",
    "    chunk_size=3, # チャンクのトークン数\n",
    "    chunk_overlap=0  # チャンクオーバーラップのトークン数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bd4e8c1-ae81-4807-9298-e82c2d8a0ffd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "18万人を超えるデータサイエンティストが参加するデータ分析のコンテスト「Kaggle（カグル）」。さまざまな分析技術を使って予測モデルを作成し、その精度の高さを競います。その世界大会が2022年に開催され、1,000チーム、5,000人を超えるデータサイエンティストが参加しました。\n",
    "\n",
    "世界中のデータサイエンティストがしのぎを削るこの大会で3位になったのが、日立製作所の諸橋政幸さん（48）が参加するチームです。データサイエンティストとして華々しい成績を収めた諸橋さんですが、ここまでの道のりは決して平たんなものではありませんでした。諸橋さんは、どのようにしてデータ分析のスペシャリストになったのか。その軌跡を追いました。\n",
    "\n",
    "「難しい問題」を解くのが好き\n",
    "インタビューに応じる諸橋政幸さん（写真：野崎航正）\n",
    "諸橋さんは子どものころから算数、数学が得意で、なかでもすぐには解けない難問に取り組むのが好きでした。大学は東北大学の物理学部に進みましたが、その理由も「物理が難しかったから、勉強してみたかったんです」と振り返ります。\n",
    "\n",
    "ただし、物理の道を究めようという気持ちはなく、就職活動の時には高校時代から興味があったプログラミングや情報処理ができる企業で働こうと考えました。ところが、希望した企業は軒並み不採用。最後の最後、「ダメもと」で受けた日立から採用通知が届きました。\n",
    "\n",
    "「当時は家電の大きな会社というイメージで、親は喜ぶだろうなと思っていましたね（笑）」\n",
    "\n",
    "セキュリティ対策を講じる部署に配属\n",
    "11歳の頃の諸橋政幸さん。この頃から問題を解くことが好きだった\n",
    "こうして1999年、日立に入社した諸橋さんは、情報システムのセキュリティを研究する部署に配属されました。当初は、暗号技術のプログラミングに関わる仕事をしたいと思っていましたが、希望とは違う仕事を担当することになったといいます。\n",
    "\n",
    "「情報システムのセキュリティ対策というと、暗号やアクセス制御などの技術的な対策が一般的ですが、実はこれだけでは不十分で、物理的な対策や、適切な運用も必要となります。そこで、統合的にセキュリティを高めるために、どんなことを検討したらいいかというプロセスや手法について研究していました」\n",
    "\n",
    "当初は思い悩んでいた時期もあったといいますが、クライアントのもとに出向いて、クライアントが抱える課題を解決するための施策を一緒に考えるようになり、やりがいを感じるようになりました。\n",
    "\n",
    "「あまり人と話すのは得意じゃないんですけど、クライアントから課題をヒアリングして、『こういう施策をしたほうがいいですよ』とアドバイスするうちに、『これは意外と面白いな』と思うようになりました」\n",
    "\n",
    "新設部署に異動も課題にぶつかる\n",
    "\n",
    "異動した当初はほとんど仕事がなかったという\n",
    "その後、諸橋さんは7年目にプラットフォームソリューション事業部、9年目に金融事業部の企画部門へと異動します。その間も、セキュリティ対策などにかかわる仕事を続けました。\n",
    "\n",
    "データサイエンティストの道に進んだきっかけは、2012年7月、データ分析を専門的に行う新設部署に異動したことでした。本人が手を挙げたわけではなく、会社が決めた人事でしたが、諸橋さんは前のめりで新しい部署に移りました。\n",
    "\n",
    "「当時、ビッグデータやデータ分析というキーワードが話題になり始めていて、興味があったんです。実際、僕が異動した1～2年後にその部署が社内公募を出したら、何十人も応募があって。選ばれた数人だけが異動できるという状況だったので、あの時期に異動できたのは本当に幸運でした」\n",
    "\n",
    "しかし、新設されたばかりの部署のため、データ分析の専門的な知識を持つ部員がいませんでした。このため、「1年目は部署で請け負っている案件が少なく、配属されたチームにはほとんど仕事がありませんでした」と諸橋さんは振り返ります。\n",
    "\n",
    "「当時は、データ分析とは何か、データで何ができるのかを把握している企業は少なかったです。最初の数年は提案しても連戦連敗で、なかなか事業が軌道に乗らず、不安を感じていましたね」\n",
    "\n",
    "データ分析への理解深め、顧客開拓\n",
    "インタビューに応じる諸橋政幸さん（写真：野崎航正）\n",
    "こうした状況を打開するために諸橋さんが考えたのが、データ分析について理解を深め、顧客を開拓するということです。「もともと新しいことを勉強するのが好き」という諸橋さんは、関連する書籍を読み漁り、データ分析のイロハをゼロから学んでいきました。\n",
    "\n",
    "「本屋にある分析の本を片っ端から、買って読みました。AI（人工知能）、機械学習、ディープラーニング、統計学…。全て学ぶのに何年かかるのか、もう大学に行って学び直したほうがいいのではないかと思ったりもしました」\n",
    "\n",
    "膨大な量の情報を前にくじけそうになったこともありますが、上司から「仕事を進めるうえで新しい技術や知識が必要になったら、その都度、学べばいい。勉強ばかりやるよりも、現場に出て経験を積んだ方が絶対にいい」とアドバイスを受け、気持ちが前向きになったといいます。\n",
    "\n",
    "こうして、データ分析に関する理解を深めていった諸橋さん。現場での経験を積もうと、部内のほかのチームに対して、「僕もプロジェクトに入れてください」と頭を下げて、無理やりチームに加えてもらうようになりました。ここから、諸橋さんの快進撃が始まります。\n",
    "\n",
    "「提案をしないと始まらない」と考えた諸橋さんは、部内だけではなく、部外との連携も強化し、さまざまな提案をしていきました。\n",
    "\n",
    "「引き合いが来たらどんな時でも対応しました。忙しくてもお金にならなくても、一度断ると次が来なくなるので。チームの人数は少なかったので、案件が増えると大変になりましたが、それでも提案資料を作って、打合せをして…。そういうことを繰り返すうちに、お客さんが耳を傾けてくれるようになりました」\n",
    "\n",
    "データサイエンティストに必要な能力\n",
    "インタビューに応じる諸橋政幸さん（写真：野崎航正）\n",
    "こうして事業が軌道に乗ってきた頃、さらなる追い風が吹きました。それがAIの普及です。AIを活用したデータ分析が競争力の源泉になるという認識が世間に広がり、データ分析への注目度が年々高まるようになりました。これに伴い、部署はかつての姿と比較できないほど多忙になりました。\n",
    "\n",
    "そして、日立は2020年、データサイエンティストのトップ人財を結集した「Lumada Data Science Lab.」を設立。データサイエンティストとして第一線で活躍するようになった諸橋さんも参加することになりました。そんな諸橋さんが、データサイエンティストの仕事の中でも特に重要と断言するのは、「ヒアリング力」です。\n",
    "\n",
    "「最初にクライアントと話をするなかで課題を特定し、そのうえでデータ分析を使って、その課題をどう解決するかを考えます。課題の特定が曖昧だったり、こちらの理解が間違っていたりすると、データ分析をする意味がなくなってしまいます。抽象的な課題を具体化して、どう分析に落とし込むかを考えるのが面白いんです」\n",
    "\n",
    "課題を特定した後は、解決するための仮説を立て、その仮説が実際に機能するかを検証します。Lumada Data Science Lab.では、ここまでのプロセスを担当し、その後システム化やサービス化を行う部署へと引き継ぎます。\n",
    "\n",
    "「ある仕事では、企業融資を行うクライアントに、『どの企業に融資するべきか、リスクを判別するモデルを作ってほしい』と依頼されました。モデル化するのには苦労しましたが、実際に運用が始まると、クライアントから『すごく効果がある』と聞き、嬉しかったですね」\n",
    "\n",
    "日立で唯一の「シニアデータデザインエキスパート」に\n",
    "「Kaggle」で世界３位に輝いた諸橋政幸さん（写真右）\n",
    "諸橋さんはいま、データサイエンティストとしてのスキルをさらに磨くため、AI開発とデータ分析の国内コンペティション「SIGNATE」や、世界を舞台にしたデータ分析のコンテスト「Kaggle」に参加するようになり、輝かしい成績を収めています。\n",
    "\n",
    "「コンペに参加する前と比べてみて、自分のデータ分析と課題解決の実力は格段に上がりました。たくさんの課題を解くことで選択肢が増えて、仮説を立てるスピードも圧倒的に速くなりましたし、他の人が1つしか案が出ないケースでも、3つ、4つの仮説が出せるようになりました」\n",
    "\n",
    "こうした実績が評価され、諸橋さんは2022年に「シニアデータデザインエキスパート」という日立で唯一の肩書きを得ました。今は普段の業務に加えて、分析技術をチーム内で共有する仕組みも作っています。\n",
    "\n",
    "「いずれ、データ分析は日常的に活用できる技術になる」と語る諸橋さん。データ分析へのニーズがますます高まるなか、生涯現役をめざして、これからもクライアントの課題解決に立ち向かいます。\n",
    "\n",
    "「試行錯誤したり、苦労したりするところが、⼀番面白いので、データ分析という仕事をずっとやっていきたいと思います」\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de59364d-aa9d-4c52-95ce-375f236b5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = text_splitter.split_text(\"今日の天気は晴れのち曇りです。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2222ad16-4622-49dc-b4d5-7f2cb03284f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今\n",
      "--------------------------------------------------------------------------------\n",
      "日の\n",
      "--------------------------------------------------------------------------------\n",
      "天気\n",
      "--------------------------------------------------------------------------------\n",
      "は\n",
      "--------------------------------------------------------------------------------\n",
      "晴\n",
      "--------------------------------------------------------------------------------\n",
      "れの\n",
      "--------------------------------------------------------------------------------\n",
      "ち\n",
      "--------------------------------------------------------------------------------\n",
      "曇\n",
      "--------------------------------------------------------------------------------\n",
      "りです\n",
      "--------------------------------------------------------------------------------\n",
      "。\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ret)):\n",
    "    print(ret[i],end=f\"\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56f373a-c39f-4e5b-a35d-ba1edc70443c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.1-py3-none-any.whl (32 kB)\n",
      "Collecting openai<2.0.0,>=1.10.0\n",
      "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.5.2\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-macosx_10_9_x86_64.whl (999 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.9/999.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain-core<0.2.0,>=0.1.33 in ./env/lib/python3.10/site-packages (from langchain-openai) (0.1.36)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai) (2.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai) (1.33)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai) (0.1.38)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./env/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai) (23.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./env/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (3.7.1)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in ./env/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./env/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.25.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./env/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./env/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.10.3)\n",
      "Requirement already satisfied: exceptiongroup in ./env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: certifi in ./env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2023.11.17)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.33->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./env/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in ./env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.33->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.33->langchain-openai) (1.26.18)\n",
      "Installing collected packages: tiktoken, openai, langchain-openai\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.5.1\n",
      "    Uninstalling tiktoken-0.5.1:\n",
      "      Successfully uninstalled tiktoken-0.5.1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.3.6\n",
      "    Uninstalling openai-1.3.6:\n",
      "      Successfully uninstalled openai-1.3.6\n",
      "Successfully installed langchain-openai-0.1.1 openai-1.14.3 tiktoken-0.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U langchain\n",
    "# !pip install -U langchain-community langchain-core langsmith\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "429a002f-12b9-46ef-83b3-a780b58da512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list | grep lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaa37f-2771-44d8-8331-4286a1e6f0a5",
   "metadata": {},
   "source": [
    "## EnsembleRetrieverの書き換え"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d0f4e-758f-4e14-ad31-94cbd5e212a2",
   "metadata": {},
   "source": [
    "### デフォルト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd6719d-154c-4744-8c3b-a32f8013bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b32f4b-4016-4afe-a45d-8d127d4a6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_1 = [\n",
    "    \"I like apples\",\n",
    "    \"I like oranges\",\n",
    "    \"Apples and oranges are fruits\",\n",
    "]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    ")\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "doc_list_2 = [\n",
    "    \"You like apples\",\n",
    "    \"You like oranges\",\n",
    "]\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list_2, embedding, metadatas=[{\"source\": 2}] * len(doc_list_2)\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a297a0dd-7d94-47c1-9abf-5440a176a5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples', metadata={'source': 1}),\n",
       " Document(page_content='You like apples', metadata={'source': 2}),\n",
       " Document(page_content='Apples and oranges are fruits', metadata={'source': 1}),\n",
       " Document(page_content='You like oranges', metadata={'source': 2})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "docs = ensemble_retriever.invoke(\"apples\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa448b40-d201-4252-85e1-e5aa5a023c58",
   "metadata": {},
   "source": [
    "### EnsembleRetrieverの上書き（取得する上位ドキュメントの指定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d45e6c5-44e2-4fa8-a1d8-1dceeab7b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 便宜上、async化されていない weighted_reciprocal_rank を書き換え\n",
    "def weighted_reciprocal_rank(\n",
    "        self, doc_lists: List[List[Document]]\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform weighted Reciprocal Rank Fusion on multiple rank lists.\n",
    "        You can find more details about RRF here:\n",
    "        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n",
    "\n",
    "        Args:\n",
    "            doc_lists: A list of rank lists, where each rank list contains unique items.\n",
    "\n",
    "        Returns:\n",
    "            list: The final aggregated list of items sorted by their weighted RRF\n",
    "                    scores in descending order.\n",
    "        \"\"\"\n",
    "        if len(doc_lists) != len(self.weights):\n",
    "            raise ValueError(\n",
    "                \"Number of rank lists must be equal to the number of weights.\"\n",
    "            )\n",
    "\n",
    "        # Create a union of all unique documents in the input doc_lists\n",
    "        all_documents = set()\n",
    "        for doc_list in doc_lists:\n",
    "            for doc in doc_list:\n",
    "                all_documents.add(doc.page_content)\n",
    "\n",
    "        # Initialize the RRF score dictionary for each document\n",
    "        rrf_score_dic = {doc: 0.0 for doc in all_documents}\n",
    "\n",
    "        # Calculate RRF scores for each document\n",
    "        for doc_list, weight in zip(doc_lists, self.weights):\n",
    "            for rank, doc in enumerate(doc_list, start=1):\n",
    "                rrf_score = weight * (1 / (rank + self.c))\n",
    "                rrf_score_dic[doc.page_content] += rrf_score\n",
    "\n",
    "        # Sort documents by their RRF scores in descending order\n",
    "        sorted_documents = sorted(\n",
    "            rrf_score_dic.keys(), key=lambda x: rrf_score_dic[x], reverse=True\n",
    "        )\n",
    "\n",
    "        # Map the sorted page_content back to the original document objects\n",
    "        page_content_to_doc_map = {\n",
    "            doc.page_content: doc for doc_list in doc_lists for doc in doc_list\n",
    "        }\n",
    "        sorted_docs = [\n",
    "            page_content_to_doc_map[page_content] for page_content in sorted_documents\n",
    "        ]\n",
    "\n",
    "        return sorted_docs[:self.doc_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e0b1b7f-ba55-4ff9-b964-3c58780d1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "class EnsembleRetriever(EnsembleRetriever):\n",
    "    retrieve_doc_num: int = 5\n",
    "    \n",
    "    def weighted_reciprocal_rank(\n",
    "        self, doc_lists: List[List[Document]]\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform weighted Reciprocal Rank Fusion on multiple rank lists.\n",
    "        You can find more details about RRF here:\n",
    "        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n",
    "\n",
    "        Args:\n",
    "            doc_lists: A list of rank lists, where each rank list contains unique items.\n",
    "\n",
    "        Returns:\n",
    "            list: The final aggregated list of items sorted by their weighted RRF\n",
    "                    scores in descending order.\n",
    "        \"\"\"\n",
    "        if len(doc_lists) != len(self.weights):\n",
    "            raise ValueError(\n",
    "                \"Number of rank lists must be equal to the number of weights.\"\n",
    "            )\n",
    "\n",
    "        # Create a union of all unique documents in the input doc_lists\n",
    "        all_documents = set()\n",
    "        for doc_list in doc_lists:\n",
    "            for doc in doc_list:\n",
    "                all_documents.add(doc.page_content)\n",
    "\n",
    "        # Initialize the RRF score dictionary for each document\n",
    "        rrf_score_dic = {doc: 0.0 for doc in all_documents}\n",
    "\n",
    "        # Calculate RRF scores for each document\n",
    "        for doc_list, weight in zip(doc_lists, self.weights):\n",
    "            for rank, doc in enumerate(doc_list, start=1):\n",
    "                rrf_score = weight * (1 / (rank + self.c))\n",
    "                rrf_score_dic[doc.page_content] += rrf_score\n",
    "\n",
    "        # Sort documents by their RRF scores in descending order\n",
    "        sorted_documents = sorted(\n",
    "            rrf_score_dic.keys(), key=lambda x: rrf_score_dic[x], reverse=True\n",
    "        )\n",
    "\n",
    "        # Map the sorted page_content back to the original document objects\n",
    "        page_content_to_doc_map = {\n",
    "            doc.page_content: doc for doc_list in doc_lists for doc in doc_list\n",
    "        }\n",
    "        sorted_docs = [\n",
    "            page_content_to_doc_map[page_content] for page_content in sorted_documents\n",
    "        ]\n",
    "\n",
    "        return sorted_docs[:self.doc_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d240e13-2d4e-4c4b-bb59-337eae80c916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples', metadata={'source': 1}),\n",
       " Document(page_content='You like apples', metadata={'source': 2})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the ensemble retriever\n",
    "EnsembleRetriever.doc_num = 2\n",
    "\n",
    "ensemble_retriever_v2 = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], \n",
    "    weights=[0.5, 0.5],\n",
    "    retrieve_doc_num=2\n",
    ")\n",
    "docs = ensemble_retriever_v2.invoke(\"apples\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc713835-5f80-4a3e-a4ff-f6a1625a2f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
