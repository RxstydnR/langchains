{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "cell_id": "0c1c7a1190a44c4da1c652f12694b8ce",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22360,
    "execution_start": 1692080227636,
    "source_hash": "a9d11aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Janome\n",
      "  Using cached Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "Installing collected packages: Janome\n",
      "Successfully installed Janome-0.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install Janome\n",
    "# !pip install rouge-score\n",
    "# !pip install bert_score --quiet\n",
    "# !pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cell_id": "b2e0f0ba05a34b6aa371b1b67d25acc8",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1692082891192,
    "source_hash": "cf469010"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge_score.tokenizers import Tokenizer\n",
    "from bert_score import BERTScorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7c8bf29b2e6b4c78b5a50a0f42d093d2",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 実行例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "cell_id": "cc5d9f65e8924200bb5134c176c4fd05",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1692083015932,
    "source_hash": "9aa26bd6"
   },
   "outputs": [],
   "source": [
    "# 要約対象\n",
    "# excerpt = \"OpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. OpenAI will build safe and beneficial AGI directly, but will also consider its mission fulfilled if its work aids others to achieve this outcome. OpenAI follows several key principles for this purpose. First, broadly distributed benefits - any influence over AGI's deployment will be used for the benefit of all, and to avoid harmful uses or undue concentration of power. Second, long-term safety - OpenAI is committed to doing the research to make AGI safe, and to promote the adoption of such research across the AI community. Third, technical leadership - OpenAI aims to be at the forefront of AI capabilities. Fourth, a cooperative orientation - OpenAI actively cooperates with other research and policy institutions, and seeks to create a global community working together to address AGI's global challenges.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人間が作成した要約の正解例\n",
    "# ref_summary = \"OpenAI aims to ensure artificial general intelligence (AGI) is used for everyone's benefit, avoiding harmful uses or undue power concentration. It is committed to researching AGI safety, promoting such studies among the AI community. OpenAI seeks to lead in AI capabilities and cooperates with global research and policy institutions to address AGI's challenges.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要約結果１\n",
    "# eval_summary_1 = \"OpenAI aims to AGI benefits all humanity, avoiding harmful uses and power concentration. It pioneers research into safe and beneficial AGI and promotes adoption globally. OpenAI maintains technical leadership in AI while cooperating with global institutions to address AGI challenges. It seeks to lead a collaborative worldwide effort developing AGI for collective good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要約結果2\n",
    "# eval_summary_2 = \"OpenAI aims to ensure AGI is for everyone's use, totally avoiding harmful stuff or big power concentration. Committed to researching AGI's safe side, promoting these studies in AI folks. OpenAI wants to be top in AI things and works with worldwide research, policy groups to figure AGI's stuff.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 要約対象\n",
    "excerpt=\"\"\"\n",
    "『葬送のフリーレン』\n",
    "終幕から始まる物語…遺されたエルフは旅の中で人の心を知っていく\n",
    "\n",
    "大勢を魅了するそのポイントは、やはり旅の中で徐々に価値観が変化していくエルフの主人公・フリーレンの姿でしょう。喪って初めて仲間の大事さを、人間という生き物の尊さを知ったフリーレン。彼女の本当の旅の始まりは、10年という短くも濃密だった冒険の後日譚から始まっていくのです。\n",
    "\n",
    "数千年を生きるエルフを動かしたのは、たった10年の旅の記憶…\n",
    "\n",
    "『葬送のフリーレン』あらすじ\n",
    "本作の主人公・フリーレンは、長寿の種族・エルフの魔法使いです。彼女は世界を救う勇者のパーティの一員として、10年の旅の末に魔王を打ち滅ぼし世界を平和に導きました。\n",
    "\n",
    "勇者・ヒンメルを始めとした人間たちには長い10年も、数千年を生きるエルフの彼女にとってはたった一瞬の出来事です。\n",
    "\n",
    "旅を終えた勇者一行は解散。冒険の後は皆、それぞれに別の道を歩むこととなりました。\n",
    "\n",
    "それから50年後。別れ際の約束通り、再び4人揃って流星群を見る事が叶った勇者一行。ですがそれから間もなく、パーティの立役者でもあった勇者・ヒンメルは大勢に惜しまれながらこの世を去りました。\n",
    "\n",
    "彼の葬式で、10年という短い歳月だった冒険の旅を思い返すフリーレン。\n",
    "\n",
    "その旅は彼女の長い人生にとってほんの一瞬の、取るに足らない些細な時間だったはず。ですが当時を思い返した時、彼女の中に押し寄せたのは「なぜその短い時間で、彼の事をもっと知ろうとしなかったのか」という後悔の感情でした。\n",
    "\n",
    "自分とは種族の違う、人間という生き物の短い生と死。かつての仲間だった彼らの命に触れて、フリーレンは人間の事をもっと知りたいと思うようになります。幸いエルフである彼女には、途方もなく長い時間が残されています。勇者一行として旅を終えた彼女は、自身を主人公とした長い長い旅路を、新たにここから始めることとなったのでした。\n",
    "\n",
    "物語は終わりから始まる…これまでにない斬新なストーリーが話題に\n",
    "\n",
    "本作が話題となったポイントのひとつが、物語の後日譚から始まるストーリーという斬新な物語構成でしょう。\n",
    "\n",
    "様々な異世界モノが現在もブームとなっていますが、どれだけめでたしめでたしで終わる物語でも、お話の中で生きる人々にはまだまだその先の人生が残っています。世界を救った勇者一行も、決して例外ではありません。\n",
    "\n",
    "ある種王道でもある、異世界でラスボスを倒し故郷に戻った勇者たちの物語。本来であれば物語の終着点から始まる今作のストーリー展開は、そのあらすじだけで大勢の読者の興味を惹きつける力のあるお話ともなっていますね。\n",
    "\n",
    "主人公らしくないフリーレンという存在から見える、人間という生き物の尊さ\n",
    "\n",
    "加えて魅力でもあるのが、ある種主人公らしくないエルフ・フリーレンの存在です。感情の起伏に乏しく淡々としたキャラクターですが、そこにはしっかりと長命の種族ならではの達観を孕んでいます。\n",
    "\n",
    "その諦めを抱えてなお、情動を強く揺さぶられた彼女の「人間を知りたい」という強い思いが、この物語の肝であることは言うまでもありません。\n",
    "\n",
    "いつの時代も一定数の憧れを集める、不老不死に近い長寿。ですがその実情はフリーレンが体現するように、様々な人々から置いていかれる孤独な存在です。そんな彼女の孤独を癒すのが、もしかしたらエルフよりずっと短い時間を懸命に生きる、人間という生き物の存在なのかもしれません。\n",
    "\n",
    "長命のエルフである彼女には当然ながら、たった数十年の寿命しかない人間の価値観を、本当の意味で理解することは難しいでしょう。\n",
    "\n",
    "けれど、それでもフリーレンは人間を知りたいと思います。\n",
    "\n",
    "その思いの根底にあるのは間違いなく、自分とは違う種族・人間への愛情。そして同時に、人間が持つ価値観や感情への憧れも。ないものねだりと知りながら、心のどこかで彼女は抱えているのかもしれませんね。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人間が作成した要約の正解例\n",
    "ref_summary = \"\"\"\n",
    "『葬送のフリーレン』は、エルフの魔法使いフリーレンが人間の心を理解していく旅の物語です。\n",
    "かつてフリーレンは、勇者ヒンメルと共に魔王を倒し世界を救う冒険をしました。\n",
    "その冒険を終えるまでに10年という月日が流れましたが、エルフの長寿からすれば人間の10年はほんの一瞬の出来事に過ぎません。\n",
    "冒険の後、勇者一行は解散し、それぞれの道を歩みました。\n",
    "\n",
    "50年後、フリーレンはヒンメルの葬式をきっかけに冒険の記憶を思い返し、なぜ彼をもっと知ろうとしなかったのだろうと後悔します。\n",
    "人間の短い命と向き合った彼女は、人間をもっと知りたいと強く願うようになります。\n",
    "こうしてフリーレンは再び新たな旅を始める、『葬送のフリーレン』はそんな旅を描いた作品です。\n",
    "\n",
    "この作品は、物語の後日譚から始まる斬新な構成が話題となりました。\n",
    "異世界ものの王道を踏まえつつ、冒険の終わりからその先を描くストーリー展開が多くの読者を魅了しています。\n",
    "\n",
    "またフリーレンが持つ人間とは違った感性に、多くの人が魅了されています。\n",
    "フリーレンは感情の起伏に乏しく、長命の種族ならではの達観を持っていますが、ヒンメルの死をきっかけに「人間を知りたい」という強い思いを抱くようになります。\n",
    "彼女は不老不死に近い長寿の孤独を抱えながらも、人間の短い時間の中で生きる姿に魅了されます。\n",
    "エルフの彼女には人間の価値観を完全に理解することは難しいものの、種族を超えた愛情と憧れを抱いています。\n",
    "\n",
    "『葬送のフリーレン』は、エルフと人間の対比を通じた人間の尊さや感情の深さを描き出しています。\n",
    "読者は、フリーレンの成長とその心情の変化、またその尊さを感じることができる作品となっています。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要約結果１(gpt-4o)\n",
    "eval_summary_1 = \"『葬送のフリーレン』は、エルフの魔法使いフリーレンが人間の感情を理解していく旅の物語です。彼女は勇者ヒンメルと共に魔王を倒し世界を救いましたが、エルフの長寿に比べ人間の10年は一瞬の出来事です。50年後、ヒンメルの死をきっかけにフリーレンは人間の短い命に触れ、人間をもっと知りたいと願うようになります。この物語は、勇者一行の冒険の後日譚から始まり、エルフの長命と人間の短命という対比を描いています。フリーレンの変化と人間の尊さへの理解が、この作品の魅力となっています。\"\n",
    "\n",
    "# 要約結果(gpt-3.5)\n",
    "eval_summary_2 = \"『葬送のフリーレン』は、旅の後の物語から始まります。エルフの魔法使いである主人公・フリーレンは、勇者のパーティと共に魔王を倒し、世界を救いました。しかし、その後、彼女は10年の旅が人間にとっては一瞬でしかないことに気付きます。友人の葬儀で彼女は後悔し、人間の生と死について知りたいと思うようになります。彼女は自分の長い生と、人間の短い寿命との違いを理解しようとしますが、同時に人間の愛情や感情にも憧れます。彼女の孤独を癒すのは、人間という存在かもしれません。\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f3ae350a2e4b47d985843c5b0808e5b6",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## ROUGEでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://eng-blog.iij.ad.jp/archives/25669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.__tokenize_stream at 0x13bc7af80>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['山田', '首相', 'は', '23', '日', 'アメリカ', 'を', '訪問', 'し', 'まし', 'た']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JanomeTokenizer(Tokenizer):\n",
    "  \"\"\"rouge-score用のJanomeを用いたTokenizerクラス\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "      from janome.tokenizer import Tokenizer\n",
    "      self.tokenizer = Tokenizer()\n",
    "      \n",
    "  def tokenize(self, text):\n",
    "      tokens = tokenizer.tokenize(text)\n",
    "      words = [token.surface for token in tokens]\n",
    "      return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiktokenTokenizer(Tokenizer):\n",
    "  \"\"\"rouge-score用のTiktokenを用いたTokenizerクラス（tokenごとのN-gram）\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    import tiktoken\n",
    "    self.enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "      \n",
    "  def tokenize(self, text):\n",
    "      return self.enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken_tokenizer = TiktokenTokenizer()\n",
    "# scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\"],tokenizer=tiktoken_tokenizer)\n",
    "janome_tokenizer = JanomeTokenizer()\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\"],tokenizer=janome_tokenizer)\n",
    "rouge_score1 = scorer.score(ref_summary, eval_summary_1)\n",
    "rouge_score2 = scorer.score(ref_summary, eval_summary_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['山田', '首相', 'は', '23', '日', 'アメリカ', 'を', '訪問', 'し', 'まし', 'た']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "# print(janome_tokenizer.tokenize(\"山田首相は23日アメリカを訪問しました\"))\n",
    "# print(janome_tokenizer.tokenize(\"首相がアメリカを訪問した\"))\n",
    "# print(scorer.score(\"山田首相は23日アメリカを訪問しました\",\"首相がアメリカを訪問した\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9dd0f_row0_col0 {\n",
       "  background-color: #83afd3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_9dd0f_row0_col1 {\n",
       "  background-color: #a5bddb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_9dd0f_row1_col0 {\n",
       "  background-color: #afc1dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_9dd0f_row1_col1 {\n",
       "  background-color: #dedcec;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9dd0f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9dd0f_level0_col0\" class=\"col_heading level0 col0\" >Summary1(GPT-4o)</th>\n",
       "      <th id=\"T_9dd0f_level0_col1\" class=\"col_heading level0 col1\" >Summary2(GPT-3.5))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9dd0f_level0_row0\" class=\"row_heading level0 row0\" >rouge1(f-score)</th>\n",
       "      <td id=\"T_9dd0f_row0_col0\" class=\"data row0 col0\" >0.463122</td>\n",
       "      <td id=\"T_9dd0f_row0_col1\" class=\"data row0 col1\" >0.375862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9dd0f_level0_row1\" class=\"row_heading level0 row1\" >rouge2(f-score)</th>\n",
       "      <td id=\"T_9dd0f_row1_col0\" class=\"data row1 col0\" >0.347676</td>\n",
       "      <td id=\"T_9dd0f_row1_col1\" class=\"data row1 col1\" >0.190311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13eaf30a0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_out = (\n",
    "    pd.DataFrame({\n",
    "        \"rouge1(f-score)\":[\n",
    "            rouge_score1[\"rouge1\"].fmeasure,\n",
    "            rouge_score2[\"rouge1\"].fmeasure\n",
    "        ],\n",
    "        \"rouge2(f-score)\":[\n",
    "            rouge_score1[\"rouge2\"].fmeasure,\n",
    "            rouge_score2[\"rouge2\"].fmeasure\n",
    "        ]\n",
    "    },index=[\"Summary1(GPT-4o)\",\"Summary2(GPT-3.5))\"]).T.style.background_gradient(vmin=0,vmax=1)\n",
    ")\n",
    "rouge_scores_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a0857e829dc64f64a183212bb5aab122",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "表は、2つの異なる要約を参照テキストに対して評価したROUGEスコアを示している。rouge-1の場合、要約2が要約1を上回っており、これは個々の単語がよりよく重複していることを示している。要約2には抜粋から直接引用された多くの単語や短いフレーズがあるため、参照要約との重なりはより高く、より高いROUGEスコアにつながる可能性が高い。\n",
    "\n",
    "ROUGEやBLEUやMETEORのような類似の測定基準は定量的な尺度を提供するが、よく作成された要約の本質を捉えられないことが多い。また、人間のスコアとの相関も悪い。流暢で首尾一貫した要約を作成することに長けているLLMの進歩を考えると、ROUGEのような従来の測定基準は、不注意にもこれらのモデルにペナルティを与えてしまう可能性がある。これは、要約の表現が異なっていても、核となる情報が正確に要約されている場合に特に当てはまる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "609cfe2cf2f14cd09e184168b83de274",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## BERTScoreでの評価\n",
    "\n",
    "ROUGEは予測文と参照文の両方における単語の正確な存在に依存しており、根本的な意味論を解釈することができない。そこでBERTScoreが登場し、BERTモデルからの文脈埋め込みを活用して、機械生成テキストの文脈における予測文と参照文の類似性を評価することを目指す。両方の文の埋め込みを比較することで、BERTScoreは従来のn-gramベースのメトリクスでは見逃される可能性のある意味的類似性を捉える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "cell_id": "b966c86ab65744f5a4a6d2e4d534c86e",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17954,
    "execution_start": 1692083196232,
    "source_hash": "a90f7d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1 F1 Score: 0.8276999592781067\n",
      "Summary 2 F1 Score: 0.7586448788642883\n"
     ]
    }
   ],
   "source": [
    "# BERTモデルの読み込み\n",
    "bert_scorer = BERTScorer(lang=\"ja\")\n",
    "\n",
    "# 評価\n",
    "P1, R1, F1_1 = bert_scorer.score([eval_summary_1], [ref_summary])\n",
    "P2, R2, F2_2 = bert_scorer.score([eval_summary_2], [ref_summary])\n",
    "print(\"Summary 1 F1 Score:\", F1_1.tolist()[0])\n",
    "print(\"Summary 2 F1 Score:\", F2_2.tolist()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "79d07eaa9a344985838133ffc9e9e02b",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "両サマリーのF値/F1スコアが近いことから、重要な情報を捕捉する上で、両サマリーは同様のパフォーマンスを示している可能性がある。しかし、このわずかな差は注意して解釈すべきである。BERTScoreは、人間の評価者が理解するような微妙な点や高度な概念を完全に把握していない可能性があるため、この指標のみに依存することは、要約の実際の品質やニュアンスを誤って解釈することにつながる可能性がある。BERTScoreと人間の判断および他の評価指標を組み合わせた統合的なアプローチは、より信頼性の高い評価を提供できるだろう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada-002で評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI APIによる埋め込みの生成\n",
    "embeddings = OpenAIEmbeddings()\n",
    "ref_emb = embeddings.embed_query(ref_summary)\n",
    "sum_emb_1 = embeddings.embed_query(eval_summary_1)\n",
    "sum_emb_2 = embeddings.embed_query(eval_summary_2)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim_1 = cosine_similarity([ref_emb],[sum_emb_1])\n",
    "cos_sim_2 = cosine_similarity([ref_emb],[sum_emb_2])\n",
    "\n",
    "print(\"Summary 1 Ada002 Similarity:\", cos_sim_1[0])\n",
    "print(\"Summary 2 Ada002 Similarity:\", cos_sim_2[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f0d66b7a59334ed3ba51d9bbbcb85890",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## GPT-4で評価\n",
    "\n",
    "5つの観点で要約を評価：\n",
    "- `正解の準備が必要`\n",
    "    - **Similarity (類似性)： AIの要約と正解の要約の類似性を評価する。**\n",
    "- `正解の準備が不要`\n",
    "    - **Relevance (関連性)： 要約が重要な情報のみを含み、余分なものを排除しているかを評価する。（※正解の準備が）**\n",
    "    - **Consistency (元文書との一貫性)： 要約が元の文書の事実と一致しているかを確認する。**\n",
    "    - **Conherence (要約内容の一貫性)：要約の論理的な流れと構成を評価。要約内で矛盾した論理展開などが発生していないか、要約の構成はあやふやじゃないか、を評価する。**\n",
    "    - Fluency (流暢さ)：要約の文法と可読性を評価する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "b029621eb5874de78b349d3cf8dd45b4",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7700,
    "execution_start": 1692083249280,
    "source_hash": "ab0afee3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_94fab_row0_col0, #T_94fab_row1_col0, #T_94fab_row1_col1, #T_94fab_row2_col0, #T_94fab_row3_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "#T_94fab_row0_col1, #T_94fab_row2_col1, #T_94fab_row3_col1 {\n",
       "  background-color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_94fab\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Summary Type</th>\n",
       "      <th id=\"T_94fab_level0_col0\" class=\"col_heading level0 col0\" >Summary 1</th>\n",
       "      <th id=\"T_94fab_level0_col1\" class=\"col_heading level0 col1\" >Summary 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Evaluation Type</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_94fab_level0_row0\" class=\"row_heading level0 row0\" >Coherence</th>\n",
       "      <td id=\"T_94fab_row0_col0\" class=\"data row0 col0\" >5</td>\n",
       "      <td id=\"T_94fab_row0_col1\" class=\"data row0 col1\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94fab_level0_row1\" class=\"row_heading level0 row1\" >Consistency</th>\n",
       "      <td id=\"T_94fab_row1_col0\" class=\"data row1 col0\" >5</td>\n",
       "      <td id=\"T_94fab_row1_col1\" class=\"data row1 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94fab_level0_row2\" class=\"row_heading level0 row2\" >Fluency</th>\n",
       "      <td id=\"T_94fab_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_94fab_row2_col1\" class=\"data row2 col1\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94fab_level0_row3\" class=\"row_heading level0 row3\" >Relevance</th>\n",
       "      <td id=\"T_94fab_row3_col0\" class=\"data row3 col0\" >5</td>\n",
       "      <td id=\"T_94fab_row3_col1\" class=\"data row3 col1\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x143907b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Evaluation prompt template based on G-Eval\\nEVALUATION_PROMPT_TEMPLATE = \\\"\\\"\\\"\\nYou will be given one summary written for an article. Your task is to rate the summary on one metric.\\nPlease make sure you read and understand these instructions very carefully. \\nPlease keep this document open while reviewing, and refer to it as needed.\\n\\nEvaluation Criteria:\\n\\n{criteria}\\n\\nEvaluation Steps:\\n\\n{steps}\\n\\nExample:\\n\\nSource Text:\\n\\n{document}\\n\\nSummary:\\n\\n{summary}\\n\\nEvaluation Form (scores ONLY):\\n\\n- {metric_name}\\n\\\"\\\"\\\"\\n\\n# Metric 1: Relevance\\n\\nRELEVANCY_SCORE_CRITERIA = \\\"\\\"\\\"\\nRelevance(1-5) - selection of important content from the source. \\\\\\nThe summary should include only important information from the source document. \\\\\\nAnnotators were instructed to penalize summaries which contained redundancies and excess information.\\n\\\"\\\"\\\"\\n\\nRELEVANCY_SCORE_STEPS = \\\"\\\"\\\"\\n1. Read the summary and the source document carefully.\\n2. Compare the summary to the source document and identify the main points of the article.\\n3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\\n4. Assign a relevance score from 1 to 5.\\n\\\"\\\"\\\"\\n\\n# Metric 2: Coherence\\n\\nCOHERENCE_SCORE_CRITERIA = \\\"\\\"\\\"\\nCoherence(1-5) - the collective quality of all sentences. \\\\\\nWe align this dimension with the DUC quality question of structure and coherence \\\\\\nwhereby \\\"the summary should be well-structured and well-organized. \\\\\\nThe summary should not just be a heap of related information, but should build from sentence to a\\\\\\ncoherent body of information about a topic.\\\"\\n\\\"\\\"\\\"\\n\\nCOHERENCE_SCORE_STEPS = \\\"\\\"\\\"\\n1. Read the article carefully and identify the main topic and key points.\\n2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\\nand if it presents them in a clear and logical order.\\n3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\\n\\\"\\\"\\\"\\n\\n# Metric 3: Consistency\\n\\nCONSISTENCY_SCORE_CRITERIA = \\\"\\\"\\\"\\nConsistency(1-5) - the factual alignment between the summary and the summarized source. \\\\\\nA factually consistent summary contains only statements that are entailed by the source document. \\\\\\nAnnotators were also asked to penalize summaries that contained hallucinated facts.\\n\\\"\\\"\\\"\\n\\nCONSISTENCY_SCORE_STEPS = \\\"\\\"\\\"\\n1. Read the article carefully and identify the main facts and details it presents.\\n2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\\n3. Assign a score for consistency based on the Evaluation Criteria.\\n\\\"\\\"\\\"\\n\\n# Metric 4: Fluency\\n\\nFLUENCY_SCORE_CRITERIA = \\\"\\\"\\\"\\nFluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\\n1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\\n2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\\n3: Good. The summary has few or no errors and is easy to read and follow.\\n\\\"\\\"\\\"\\n\\nFLUENCY_SCORE_STEPS = \\\"\\\"\\\"\\nRead the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.\\n\\\"\\\"\\\"\\n\\n\\ndef get_geval_score(\\n    criteria: str, steps: str, document: str, summary: str, metric_name: str\\n):\\n    prompt = EVALUATION_PROMPT_TEMPLATE.format(\\n        criteria=criteria,\\n        steps=steps,\\n        metric_name=metric_name,\\n        document=document,\\n        summary=summary,\\n    )\\n    response = openai.chat.completions.create(\\n        model=\\\"gpt-4\\\",\\n        messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": prompt}],\\n        temperature=0,\\n        max_tokens=5,\\n        top_p=1,\\n        frequency_penalty=0,\\n        presence_penalty=0,\\n    )\\n    return response.choices[0].message.content\\n\\n\\nevaluation_metrics = {\\n    \\\"Relevance\\\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\\n    \\\"Coherence\\\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\\n    \\\"Consistency\\\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\\n    \\\"Fluency\\\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\\n}\\n\\nsummaries = {\\\"Summary 1\\\": eval_summary_1, \\\"Summary 2\\\": eval_summary_2}\\n\\ndata = {\\\"Evaluation Type\\\": [], \\\"Summary Type\\\": [], \\\"Score\\\": []}\\n\\nfor eval_type, (criteria, steps) in evaluation_metrics.items():\\n    for summ_type, summary in summaries.items():\\n        data[\\\"Evaluation Type\\\"].append(eval_type)\\n        data[\\\"Summary Type\\\"].append(summ_type)\\n        result = get_geval_score(criteria, steps, excerpt, summary, eval_type)\\n        score_num = int(result.strip())\\n        data[\\\"Score\\\"].append(score_num)\\n\\npivot_df = pd.DataFrame(data, index=None).pivot(\\n    index=\\\"Evaluation Type\\\", columns=\\\"Summary Type\\\", values=\\\"Score\\\"\\n)\\nstyled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\\ndisplay(styled_pivot_df)\";\n",
       "                var nbb_formatted_code = \"# Evaluation prompt template based on G-Eval\\nEVALUATION_PROMPT_TEMPLATE = \\\"\\\"\\\"\\nYou will be given one summary written for an article. Your task is to rate the summary on one metric.\\nPlease make sure you read and understand these instructions very carefully. \\nPlease keep this document open while reviewing, and refer to it as needed.\\n\\nEvaluation Criteria:\\n\\n{criteria}\\n\\nEvaluation Steps:\\n\\n{steps}\\n\\nExample:\\n\\nSource Text:\\n\\n{document}\\n\\nSummary:\\n\\n{summary}\\n\\nEvaluation Form (scores ONLY):\\n\\n- {metric_name}\\n\\\"\\\"\\\"\\n\\n# Metric 1: Relevance\\n\\nRELEVANCY_SCORE_CRITERIA = \\\"\\\"\\\"\\nRelevance(1-5) - selection of important content from the source. \\\\\\nThe summary should include only important information from the source document. \\\\\\nAnnotators were instructed to penalize summaries which contained redundancies and excess information.\\n\\\"\\\"\\\"\\n\\nRELEVANCY_SCORE_STEPS = \\\"\\\"\\\"\\n1. Read the summary and the source document carefully.\\n2. Compare the summary to the source document and identify the main points of the article.\\n3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\\n4. Assign a relevance score from 1 to 5.\\n\\\"\\\"\\\"\\n\\n# Metric 2: Coherence\\n\\nCOHERENCE_SCORE_CRITERIA = \\\"\\\"\\\"\\nCoherence(1-5) - the collective quality of all sentences. \\\\\\nWe align this dimension with the DUC quality question of structure and coherence \\\\\\nwhereby \\\"the summary should be well-structured and well-organized. \\\\\\nThe summary should not just be a heap of related information, but should build from sentence to a\\\\\\ncoherent body of information about a topic.\\\"\\n\\\"\\\"\\\"\\n\\nCOHERENCE_SCORE_STEPS = \\\"\\\"\\\"\\n1. Read the article carefully and identify the main topic and key points.\\n2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\\nand if it presents them in a clear and logical order.\\n3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\\n\\\"\\\"\\\"\\n\\n# Metric 3: Consistency\\n\\nCONSISTENCY_SCORE_CRITERIA = \\\"\\\"\\\"\\nConsistency(1-5) - the factual alignment between the summary and the summarized source. \\\\\\nA factually consistent summary contains only statements that are entailed by the source document. \\\\\\nAnnotators were also asked to penalize summaries that contained hallucinated facts.\\n\\\"\\\"\\\"\\n\\nCONSISTENCY_SCORE_STEPS = \\\"\\\"\\\"\\n1. Read the article carefully and identify the main facts and details it presents.\\n2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\\n3. Assign a score for consistency based on the Evaluation Criteria.\\n\\\"\\\"\\\"\\n\\n# Metric 4: Fluency\\n\\nFLUENCY_SCORE_CRITERIA = \\\"\\\"\\\"\\nFluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\\n1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\\n2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\\n3: Good. The summary has few or no errors and is easy to read and follow.\\n\\\"\\\"\\\"\\n\\nFLUENCY_SCORE_STEPS = \\\"\\\"\\\"\\nRead the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.\\n\\\"\\\"\\\"\\n\\n\\ndef get_geval_score(\\n    criteria: str, steps: str, document: str, summary: str, metric_name: str\\n):\\n    prompt = EVALUATION_PROMPT_TEMPLATE.format(\\n        criteria=criteria,\\n        steps=steps,\\n        metric_name=metric_name,\\n        document=document,\\n        summary=summary,\\n    )\\n    response = openai.chat.completions.create(\\n        model=\\\"gpt-4\\\",\\n        messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": prompt}],\\n        temperature=0,\\n        max_tokens=5,\\n        top_p=1,\\n        frequency_penalty=0,\\n        presence_penalty=0,\\n    )\\n    return response.choices[0].message.content\\n\\n\\nevaluation_metrics = {\\n    \\\"Relevance\\\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\\n    \\\"Coherence\\\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\\n    \\\"Consistency\\\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\\n    \\\"Fluency\\\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\\n}\\n\\nsummaries = {\\\"Summary 1\\\": eval_summary_1, \\\"Summary 2\\\": eval_summary_2}\\n\\ndata = {\\\"Evaluation Type\\\": [], \\\"Summary Type\\\": [], \\\"Score\\\": []}\\n\\nfor eval_type, (criteria, steps) in evaluation_metrics.items():\\n    for summ_type, summary in summaries.items():\\n        data[\\\"Evaluation Type\\\"].append(eval_type)\\n        data[\\\"Summary Type\\\"].append(summ_type)\\n        result = get_geval_score(criteria, steps, excerpt, summary, eval_type)\\n        score_num = int(result.strip())\\n        data[\\\"Score\\\"].append(score_num)\\n\\npivot_df = pd.DataFrame(data, index=None).pivot(\\n    index=\\\"Evaluation Type\\\", columns=\\\"Summary Type\\\", values=\\\"Score\\\"\\n)\\nstyled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\\ndisplay(styled_pivot_df)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation prompt template based on G-Eval\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "# Metric 1: Relevance\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "Relevance(1-5) - selection of important content from the source. \\\n",
    "The summary should include only important information from the source document. \\\n",
    "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
    "Coherence(1-5) - the collective quality of all sentences. \\\n",
    "We align this dimension with the DUC quality question of structure and coherence \\\n",
    "whereby \"the summary should be well-structured and well-organized. \\\n",
    "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
    "coherent body of information about a topic.\"\n",
    "\"\"\"\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main topic and key points.\n",
    "2. Read the summary and compare it to the article. Check if the summary covers the main topic and key points of the article,\n",
    "and if it presents them in a clear and logical order.\n",
    "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
    "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
    "A factually consistent summary contains only statements that are entailed by the source document. \\\n",
    "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
    "\"\"\"\n",
    "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the article carefully and identify the main facts and details it presents.\n",
    "2. Read the summary and compare it to the article. Check if the summary contains any factual errors that are not supported by the article.\n",
    "3. Assign a score for consistency based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 4: Fluency\n",
    "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "3: Good. The summary has few or no errors and is easy to read and follow.\n",
    "\"\"\"\n",
    "FLUENCY_SCORE_STEPS = \"\"\"\n",
    "Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 3.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, document: str, summary: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        metric_name=metric_name,\n",
    "        document=document,\n",
    "        summary=summary,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
    "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"Summary 1\": eval_summary_1, \"Summary 2\": eval_summary_2}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, excerpt, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")\n",
    "styled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\n",
    "display(styled_pivot_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "03a7682297624a71ae448cf67192d8fd",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 0
    },
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "全体的に、要約1は4つのカテゴリーのうち3つ（一貫性、関連性、流暢さ）で要約2を上回っているようである。両要約は互いに一貫(要約が元の文書の事実と一致)していることがわかった。この結果は、与えられた評価基準に基づいて、要約1の方が一般的に望ましいことを示唆しているかもしれない。\n",
    "\n",
    "### 制限事項\n",
    " LLMベースのメトリクスは、人間が書いたテキストよりもLLMが生成したテキストを好む傾向があることに注意。\n",
    " さらに、LLMベースのメトリクスは、システム・メッセージ／プロンプトの影響を受けやすい。\n",
    " 私たちは、パフォーマンスを向上させ、一貫したスコアを得るのに役立つ他のテクニックを試すことを推奨する。\n",
    " また、このスコアリング手法は現在のところgpt-4のコンテキストウィンドウによって制限されていることは注目に値する。\n",
    "\n",
    "\n",
    "### 結論\n",
    "抽象的要約の評価は、さらなる改善の余地がある。ROUGE、BLEUScore、BERTScoreのような従来の評価基準は、有用な自動評価を提供するが、意味的類似性や要約品質の微妙な側面を捉えることには限界がある。さらに、これらは参照出力を必要とするが、その収集やラベル付けにはコストがかかる。LLMベースのメトリクスは、首尾一貫性、流暢さ、関連性を評価する参照不要の方法として有望である。しかし、これらにも、LLMによって生成されたテキストを好む潜在的なバイアスがある。結局のところ、抽象的要約システムを確実に評価するには、自動評価基準と人間による評価の組み合わせが理想的である。要約の質を包括的に理解するためには人間による評価が不可欠であるが、効率的で大規模なテストを可能にするためには、自動評価で補完されるべきである。この分野は、品質、スケーラビリティ、公平性のバランスをとりながら、より強固な評価手法を進化させ続けるだろう。評価手法を進化させることは、プロダクションアプリケーションの進歩を推進する上で極めて重要である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_app_layout": "powerful-article",
  "deepnote_execution_queue": [],
  "deepnote_full_width": true,
  "deepnote_notebook_id": "20f885ddefe84c16bd1250151b5a5e1f",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
